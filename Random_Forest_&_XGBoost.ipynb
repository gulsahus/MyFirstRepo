{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnMsrTHG6i9m52Xv2FPOWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulsahus/MyFirstRepo/blob/main/Random_Forest_%26_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35O18aR88yKB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Data Loading and Initial Exploration**"
      ],
      "metadata": {
        "id": "OOyG4TcE9hmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, KBinsDiscretizer, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import shap\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "pWOCFJZu-HOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "kXkGgNJ094Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pickle file\n",
        "file_path = '/Users/gulsah/Desktop/df8.pkl'\n",
        "df = pd.read_pickle(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "wLoX-sbi-Fnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inspect Data"
      ],
      "metadata": {
        "id": "54sTp_FV-BEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows and columns\n",
        "rows, columns = df.shape\n",
        "print(f'The DataFrame has {rows} rows and {columns} columns.')"
      ],
      "metadata": {
        "id": "r0q2NfZJ-MOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filter Data\n",
        "Filter the DataFrame for specific years (2022, 2023, 2024) due to high running time of the models\n"
      ],
      "metadata": {
        "id": "eiFRYFD1-C2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame for the desired years\n",
        "desired_years = [2024,2023,2022]\n",
        "df = df[df['CalYear'].isin(desired_years)]\n",
        "\n",
        "#The same analyis was repeated for 2014-2018 & 2019-2024 time frames."
      ],
      "metadata": {
        "id": "qpaqvlGE-PKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "70-UB8ko9nt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define Columns for Transformations"
      ],
      "metadata": {
        "id": "65ZSCghL-Yk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define columns for different transformations\n",
        "min_max_cols = [\"NumCalls\"]\n",
        "std_scale_cols = [\"proj_Air_Distance_rounded\", \"proj_PastIncidentsCount\", 'proj_CityCenter', 'proj_PeakHours']\n",
        "one_hot_cols = [\"proj_CalMonth\", \"IncidentGroup\", \"StopCodeDescription\", \"PropertyCategory\",\n",
        "                \"IncGeo_BoroughName\",\"proj_CalWeekday\", \"proj_WindDescription\", \"SpecialServiceType\"]\n",
        "cyclical_cols = ['proj_Day_of_Year', 'HourOfCall']"
      ],
      "metadata": {
        "id": "CRgOO4Bu-qNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply One-Hot Encoding\n",
        "Encode categorical variables using one-hot encoding and concatenate the results to the DataFrame"
      ],
      "metadata": {
        "id": "fwNgFK8T-feO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply One-Hot Encoding\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoded_df = pd.DataFrame(encoder.fit_transform(df[one_hot_cols]))\n",
        "encoded_df.columns = encoder.get_feature_names_out(one_hot_cols)\n",
        "\n",
        "# Drop original one-hot encoded columns and concatenate encoded columns\n",
        "df = df.drop(one_hot_cols, axis=1)\n",
        "df = pd.concat([df, encoded_df], axis=1)"
      ],
      "metadata": {
        "id": "8UP7A9Pj-q-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cyclical Transformations\n",
        "Apply sine and cosine transformations to time-based features to capture cyclical patterns."
      ],
      "metadata": {
        "id": "gOetAIwC-hdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cyclical transformations\n",
        "df['day_of_year_sin'] = np.sin(2 * np.pi * df['proj_Day_of_Year'] / 366)\n",
        "df['day_of_year_cos'] = np.cos(2 * np.pi * df['proj_Day_of_Year'] / 366)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['HourOfCall'] / 24)\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['HourOfCall'] / 24)\n"
      ],
      "metadata": {
        "id": "tiKSc0LI-rl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert Columns to Numeric"
      ],
      "metadata": {
        "id": "v85_je9E-jZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert non-numeric columns to numeric\n",
        "for col in std_scale_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "7Vtotg3i-sKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Select Final Columns\n",
        "Identify the final set of columns to be used for analysis."
      ],
      "metadata": {
        "id": "XVp3LKOG-lTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all desired columns are present and combined properly\n",
        "final_columns = min_max_cols + std_scale_cols + list(encoded_df.columns) + ['TravelTimeSeconds', 'day_of_year_sin', 'day_of_year_cos', 'hour_cos', 'hour_sin']\n",
        "df_final = df[final_columns]\n"
      ],
      "metadata": {
        "id": "vYJ4XllE-soC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handle Missing Values"
      ],
      "metadata": {
        "id": "j2V2A9Vf-nwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values again after all transformations\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_final_imputed = pd.DataFrame(imputer.fit_transform(df_final), columns=df_final.columns)\n"
      ],
      "metadata": {
        "id": "0O3IyaaM-tHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering**\n"
      ],
      "metadata": {
        "id": "zcyG3-jG9tlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert Travel Time\n",
        "Convert travel time from seconds to minutes."
      ],
      "metadata": {
        "id": "-ibPb_XS-_T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically create bins with equal frequency\n",
        "df_final_imputed['TravelTimeMinutes'] = df_final_imputed['TravelTimeSeconds'] / 60"
      ],
      "metadata": {
        "id": "MNlHfR2IBGvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Distributions\n",
        "Plot histograms and box plots to visualize the distribution of travel times."
      ],
      "metadata": {
        "id": "1dN5fKQY_CvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of TravelTimeMinutes\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_final_imputed['TravelTimeMinutes'], bins=50, kde=True)\n",
        "plt.title('Distribution of TravelTimeMinutes')\n",
        "plt.xlabel('TravelTimeMinutes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Use a box plot to see the distribution and outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=df_final_imputed['TravelTimeMinutes'])\n",
        "plt.title('Box Plot of TravelTimeMinutes')\n",
        "plt.xlabel('TravelTimeMinutes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RlqJBUvABLPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check for Repeated Values\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d2ayc0pH_FTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for repeated values\n",
        "travel_time_counts = df_final_imputed['TravelTimeMinutes'].value_counts().sort_values(ascending=False)\n",
        "print(travel_time_counts.head(10))  # Display the top 10 most frequent values\n"
      ],
      "metadata": {
        "id": "zHUojyupBNvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Log Transformation (optional)"
      ],
      "metadata": {
        "id": "pHli3_P9_HKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation\n",
        "df_final_imputed['TravelTimeMinutesLog'] = np.log1p(df_final_imputed['TravelTimeMinutes'])"
      ],
      "metadata": {
        "id": "guvqaJlFBQKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Discretize Travel Time\n",
        "\n",
        "Bin the log-transformed travel time into categories."
      ],
      "metadata": {
        "id": "bbVYA94O_J9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_bins = 8\n",
        "\n",
        "# Use KBinsDiscretizer with transformed data\n",
        "discretizer = KBinsDiscretizer(n_bins=k_bins, encode='ordinal', strategy='quantile')\n",
        "df_final_imputed['TravelTimeCategoryLog'] = discretizer.fit_transform(df_final_imputed[['TravelTimeMinutesLog']])\n",
        "\n",
        "# Convert numeric bins to labels\n",
        "df_final_imputed['TravelTimeCategoryLog'] = df_final_imputed['TravelTimeCategoryLog'].apply(lambda x: f'Bin{x+1}')\n"
      ],
      "metadata": {
        "id": "qWEbS7yWBWAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Binned Data\n",
        "Display summary counts of the binned categories and visualize bin edges.\n"
      ],
      "metadata": {
        "id": "wQTo6787_O3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of bin counts for transformed data\n",
        "bin_counts_log = df_final_imputed['TravelTimeCategoryLog'].value_counts()\n",
        "print(\"Summary of TravelTimeCategoryLog bin counts:\")\n",
        "print(bin_counts_log)\n",
        "\n",
        "# Display the bin edges to understand which transformed minutes fall into each bin\n",
        "bin_edges_log = discretizer.bin_edges_[0]\n",
        "for i in range(len(bin_edges_log) - 1):\n",
        "    print(f\"Bin {i+1}: {bin_edges_log[i]} to {bin_edges_log[i+1]}\")"
      ],
      "metadata": {
        "id": "voChJ1bZBfOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drop Unnecessary Columns\n",
        "Remove columns that are no longer needed after transformations."
      ],
      "metadata": {
        "id": "Qnfc1Yay_Q4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['proj_Day_of_Year', 'HourOfCall', 'TravelTimeSeconds', 'TravelTimeMinutes']\n",
        "df_final_imputed = df_final_imputed.drop(columns=[col for col in columns_to_drop if col in df_final_imputed.columns])\n"
      ],
      "metadata": {
        "id": "f2oQ3VFMBhMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final NaN Check\n",
        "Ensure there are no remaining NaN values in the DataFrame."
      ],
      "metadata": {
        "id": "DD9FiOX7_Sh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any remaining NaN values in the dataset\n",
        "nan_counts = df_final_imputed.isnull().sum()\n",
        "nan_counts_filtered = nan_counts[nan_counts > 0]\n",
        "if not nan_counts_filtered.empty:\n",
        "    print(\"Columns with NaN values after all preprocessing:\")\n",
        "    print(nan_counts_filtered)\n",
        "    # Handle any remaining NaNs\n",
        "    df_final_imputed = pd.DataFrame(imputer.fit_transform(df_final_imputed), columns=df_final_imputed.columns)\n",
        "else:\n",
        "    print(\"No NaN values found in the DataFrame after all preprocessing.\")\n"
      ],
      "metadata": {
        "id": "8_3xTk7YBnG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Preparation**"
      ],
      "metadata": {
        "id": "iDJNqac59xKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare Data for Modeling"
      ],
      "metadata": {
        "id": "3-rF_rmi_X2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model preparation\n",
        "X = df_final_imputed.drop(['TravelTimeCategoryLog'], axis=1)\n",
        "y = df_final_imputed['TravelTimeCategoryLog']\n"
      ],
      "metadata": {
        "id": "84jxrBdHBsNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train-Test Split"
      ],
      "metadata": {
        "id": "hjr-Ys1W_akj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Rg1nHRbbBu3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encode Target Variable"
      ],
      "metadata": {
        "id": "SQEkPbVQ_dOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n"
      ],
      "metadata": {
        "id": "LhGR2IlABwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scale Features"
      ],
      "metadata": {
        "id": "yp3bx8MX_g16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n"
      ],
      "metadata": {
        "id": "Qda8IlWbBzOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection - Variance Threshold & SeleckKBest\n",
        "Select features based on variance threshold to remove low-variance features."
      ],
      "metadata": {
        "id": "mdpBji2h_i72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection using VarianceThreshold with threshold of 0.05\n",
        "threshold = 0.05\n",
        "sel_variance = VarianceThreshold(threshold=threshold)\n",
        "sel_variance.fit(X_train_scaled)\n",
        "X_train_filtered = X_train_scaled.loc[:, sel_variance.get_support()]\n",
        "X_test_filtered = X_test_scaled.loc[:, sel_variance.get_support()]\n",
        "\n",
        "# Feature selection using SelectKBest with mutual_info_regression\n",
        "sel_kbest = SelectKBest(score_func=mutual_info_regression, k=20)\n",
        "sel_kbest.fit(X_train_filtered, y_train_encoded)\n",
        "\n",
        "# Applying the transformation to the train and test sets\n",
        "X_train_selected = X_train_filtered.loc[:, sel_kbest.get_support()]\n",
        "X_test_selected = X_test_filtered.loc[:, sel_kbest.get_support()]\n",
        "\n",
        "# Retrieve the selected feature names\n",
        "selected_features_final = X_train_filtered.columns[sel_kbest.get_support()]\n"
      ],
      "metadata": {
        "id": "4IDcTyb3B5Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Selected Features"
      ],
      "metadata": {
        "id": "LGDfb2qY_m7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualization of feature selection\n",
        "plt.matshow(sel_kbest.get_support().reshape(1, -1), cmap='gray_r')\n",
        "plt.xlabel('Features Index')\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "# Print the final selected features\n",
        "print(\"Selected features:\", selected_features_final.tolist())"
      ],
      "metadata": {
        "id": "-RC8EtD6B9BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modeling**\n"
      ],
      "metadata": {
        "id": "0qhBcWGp90eM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RandomForest Model\n",
        "Train a RandomForest model on the selected features and evaluate its performance using a classification report."
      ],
      "metadata": {
        "id": "k-W66Rl2_qJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model training and evaluation with RandomForest\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_rf.fit(X_train_selected, y_train_encoded)\n",
        "y_pred_rf_encoded = model_rf.predict(X_test_selected)\n",
        "\n",
        "# Decode the predicted labels for RandomForest\n",
        "y_pred_rf = label_encoder.inverse_transform(y_pred_rf_encoded)\n",
        "\n",
        "# Classification report for RandomForest\n",
        "print(\"RandomForest Classification Report\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "ra8xbp8fCCtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGBoost Model\n",
        "Train an XGBoost model on the selected features and evaluate its performance using a classification report."
      ],
      "metadata": {
        "id": "t0bDWTZ0_sNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model training and evaluation with XGBoost\n",
        "model_xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model_xgb.fit(X_train_selected, y_train_encoded)\n",
        "y_pred_xgb_encoded = model_xgb.predict(X_test_selected)\n",
        "\n",
        "# Decode the predicted labels for XGBoost\n",
        "y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)\n",
        "\n",
        "# Classification report for XGBoost\n",
        "print(\"XGBoost Classification Report\")\n",
        "print(classification_report(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "Vh44BgTGCFH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Importances**"
      ],
      "metadata": {
        "id": "zJvYvOcyCHQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Feature Importance"
      ],
      "metadata": {
        "id": "oB-eZ9k0CMY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from the RandomForest model\n",
        "rf_importances = model_rf.feature_importances_\n",
        "rf_feature_names = selected_features_final\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "rf_importances_df = pd.DataFrame({\n",
        "    'Feature': rf_feature_names,\n",
        "    'Importance': rf_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(rf_importances_df['Feature'], rf_importances_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('RandomForest Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cvBsUdoWCZNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost Feature Importance"
      ],
      "metadata": {
        "id": "1KaFqcv0CQEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get feature importances from the XGBoost model\n",
        "xgb_importances = model_xgb.feature_importances_\n",
        "xgb_feature_names = selected_features_final\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "xgb_importances_df = pd.DataFrame({\n",
        "    'Feature': xgb_feature_names,\n",
        "    'Importance': xgb_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(xgb_importances_df['Feature'], xgb_importances_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('XGBoost Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w_QRY7D9CWNR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}